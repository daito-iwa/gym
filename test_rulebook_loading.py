#!/usr/bin/env python3
"""
ãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
OpenAI APIã‚­ãƒ¼ãªã—ã§PDFã®å†…å®¹ç¢ºèªã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ãƒ†ã‚¹ãƒˆ
"""

import pdfplumber
import warnings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

warnings.filterwarnings('ignore')

def test_pdf_loading():
    """PDFã®èª­ã¿è¾¼ã¿ã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ… ä½“æ“AIãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # æ—¥æœ¬èªãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯ã®ãƒ†ã‚¹ãƒˆ
    pdf_path = "data/rulebook_ja.pdf"
    
    try:
        all_docs = []
        
        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            print(f"ğŸ“‹ ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸")
            
            for i, page in enumerate(pdf.pages[:5]):  # æœ€åˆã®5ãƒšãƒ¼ã‚¸ã‚’ãƒ†ã‚¹ãƒˆ
                text = page.extract_text() or ""
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
                tables = page.extract_tables()
                tables_md = ""
                if tables:
                    for table in tables:
                        clean_table = [[str(cell) if cell is not None else "" for cell in row] for row in table]
                        if clean_table:
                            header = "| " + " | ".join(clean_table[0]) + " |"
                            separator = "| " + " | ".join(["---"] * len(clean_table[0])) + " |"
                            rows = "\n".join(["| " + " | ".join(row) + " |" for row in clean_table[1:]])
                            tables_md += f"\n{header}\n{separator}\n{rows}\n"
                
                page_content = text + tables_md
                metadata = {"source": pdf_path, "page": i}
                doc = Document(page_content=page_content, metadata=metadata)
                all_docs.append(doc)
                
                print(f"  ãƒšãƒ¼ã‚¸ {i+1}: {len(text)}æ–‡å­—, ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables) if tables else 0}")
        
        print(f"\nğŸ“Š èª­ã¿è¾¼ã¿å®Œäº†: {len(all_docs)}ãƒšãƒ¼ã‚¸")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ†ã‚¹ãƒˆ
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=300,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        
        texts = text_splitter.split_documents(all_docs)
        print(f"ğŸ”„ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²çµæœ: {len(texts)}ãƒãƒ£ãƒ³ã‚¯")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ã‚¯ã®è¡¨ç¤º
        if texts:
            sample_chunk = texts[0]
            print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ã‚¯:")
            print(f"  æ–‡å­—æ•°: {len(sample_chunk.page_content)}")
            print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {sample_chunk.metadata}")
            print(f"  å†…å®¹: {sample_chunk.page_content[:300]}...")
        
        # ä½“æ“æŠ€è¡“é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ†ã‚¹ãƒˆ:")
        keywords = ["é‰„æ£’", "æ‰‹æ”¾ã—æŠ€", "é›£åº¦", "æ¸›ç‚¹", "ç€åœ°"]
        
        for keyword in keywords:
            found_chunks = [chunk for chunk in texts if keyword in chunk.page_content]
            print(f"  '{keyword}': {len(found_chunks)}ãƒãƒ£ãƒ³ã‚¯")
            
            if found_chunks:
                sample = found_chunks[0].page_content
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
                idx = sample.find(keyword)
                if idx != -1:
                    start = max(0, idx - 50)
                    end = min(len(sample), idx + 100)
                    context = sample[start:end].replace('\n', ' ')
                    print(f"    ä¾‹: ...{context}...")
        
        print(f"\nâœ… ãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        print(f"ğŸ’¡ å®Ÿéš›ã®AIæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    test_pdf_loading()